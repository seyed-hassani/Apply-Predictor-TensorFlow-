{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c505bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02db22ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('admission_train.csv') # TODO: load the admission_train.csv file\n",
    "test = pd.read_csv('admission_test.csv')  # TODO: load the admission_test.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d58e3a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ccdfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e018332",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaaa12ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = tf.convert_to_tensor(train) # TODO: convert the train dataframe to a tensor format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b860a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = tf.random.shuffle(train_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64cda32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = 50  \n",
    "\n",
    "train_data = train_tf[VALIDATION_SIZE:]         \n",
    "validation_data = train_tf[:VALIDATION_SIZE]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c01118e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", train_data[1][:-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad46e250",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(train_data)\n",
    "X_train = train_data[:, :-1]  # همه‌ی ویژگی‌ها\n",
    "y_train = train_data[:, -1]   # فقط برچسب (label)\n",
    "\n",
    "validation_data = np.array(validation_data)\n",
    "X_val = validation_data[:, :-1]\n",
    "y_val = validation_data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e71cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tf.convert_to_tensor(test) # TODO: convert the test dataframe to a tensor format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82ca5b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer(tf.Module):\n",
    "  def __init__(self):\n",
    "    self.mean = None\n",
    "    self.std = None\n",
    "\n",
    "  def fit(self, x):\n",
    "    self.mean = tf.math.reduce_mean(x,axis=0) # TODO: calculate the mean of the input using tf.math.reduce_mean\n",
    "    self.std = tf.math.reduce_std(x,axis=0) # TODO: calculate the standard deviation of the input using tf.math.reduce_std\n",
    "\n",
    "  def transform(self, x):\n",
    "    return (x-self.mean)/self.std # TODO: normalize the input by subtracting the mean and dividing by the standard deviation\n",
    "\n",
    "  def fit_transform(self, x):\n",
    "    self.fit(x)# first call the fit method to calculate the mean and standard deviation\n",
    "    return self.transform(x) # TODO: then call the transform method to normalize the input and return the result\n",
    "\n",
    "  def inverse_transform(self, x):\n",
    "    return (x*self.std)+self.mean # TODO: denormalize the input by multiplying by the standard deviation and adding the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02f31205",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2e120ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "\n",
    "X_train_norm = normalizer.fit_transform(X_train) # TODO: fit the normalizer on the training data and then transform it to get the normalized version\n",
    "X_val_norm = normalizer.transform(X_val)\n",
    "X_test_norm = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75ea1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(tf.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    self.built = False\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self, x):\n",
    "    if not self.built:\n",
    "      input_dim = tf.shape(x)[1]  # به صورت داینامیک تعداد ویژگی‌ها را بگیر\n",
    "      rand_w = tf.random.uniform(shape=[input_dim, 1], dtype=tf.float32)\n",
    "      rand_b = tf.random.uniform(shape=[], dtype=tf.float32)\n",
    "      self.w = tf.Variable(rand_w)\n",
    "      self.b = tf.Variable(rand_b)\n",
    "      self.built = True\n",
    "\n",
    "    x = tf.cast(x, tf.float32)  # اطمینان از تطابق نوع داده\n",
    "    y = tf.add(tf.matmul(x, self.w), self.b)\n",
    "    output = tf.squeeze(y, axis=1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6c9af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b38a9d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor(X_train_norm[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ef1f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e829fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10 # TODO: set the batch size to your preference\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_norm, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=X_train_norm.shape[0]).batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_norm, y_val))\n",
    "val_dataset = val_dataset.shuffle(buffer_size=X_val_norm.shape[0]).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe9595c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "EPOCHS = 10 # TODO: set the number of epochs to your preference\n",
    "LR = 0.001 # TODO: set the learning rate to your preference\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "# create the model\n",
    "regressor = LinearRegression()\n",
    "\n",
    "# train the model\n",
    "for epoch in range(EPOCHS):\n",
    "  batch_losses_train, batch_losses_val = [], []\n",
    "\n",
    "  # iterate through the training data\n",
    "  for X_batch, y_batch in train_dataset:\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred_batch = regressor(X_batch) # TODO: pass current batch to the regressor to get the predictions\n",
    "        batch_loss = mse_loss(y_batch,y_pred_batch) # TODO: calculate the loss using the mse_loss function\n",
    "\n",
    "    # get the gradients of the weights with respect to the loss\n",
    "    grads = tape.gradient(batch_loss, regressor.variables)\n",
    "\n",
    "    # update the weights of the model\n",
    "    for g, v in zip(grads, regressor.variables):\n",
    "      v.assign_sub(LR*g) # TODO: update the variable by subtracting the product of the gradient and the learning rate using v.assign_sub\n",
    "    \n",
    "    # keep track of batch-level training performance \n",
    "    batch_losses_train.append(batch_loss)\n",
    "\n",
    "  # iterate through the validation data\n",
    "  for X_batch, y_batch in val_dataset:\n",
    "\n",
    "    y_pred_batch = regressor(X_batch) # TODO: pass current batch to the regressor to get the predictions\n",
    "    batch_loss = mse_loss(y_batch,y_pred_batch) # TODO: calculate the loss using the mse_loss function\n",
    "\n",
    "    # keep track of batch-level validation performance \n",
    "    batch_losses_val.append(batch_loss)\n",
    "\n",
    "  # keep track of epoch-level model performance\n",
    "  train_loss = tf.reduce_mean(batch_losses_train)\n",
    "  val_loss = tf.reduce_mean(batch_losses_val)\n",
    "  train_losses.append(train_loss)\n",
    "  val_losses.append(val_loss)\n",
    "  if epoch % 10 == 0:\n",
    "    print(f'Mean squared error for epoch {epoch}: {train_loss.numpy():0.3f}')\n",
    "\n",
    "# output final losses\n",
    "print(f\"\\nFinal train loss: {train_loss:0.3f}\")\n",
    "print(f\"Final validation loss: {val_loss:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac2b3dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "320d1470",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10 # TODO: set the batch size to your preference\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_norm, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=X_train_norm.shape[0]).batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_norm, y_val))\n",
    "val_dataset = val_dataset.shuffle(buffer_size=X_val_norm.shape[0]).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6d5cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "EPOCHS = 10 # TODO: set the number of epochs to your preference\n",
    "LR = 0.001 # TODO: set the learning rate to your preference\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "# create the model\n",
    "regressor = LinearRegression()\n",
    "\n",
    "# train the model\n",
    "for epoch in range(EPOCHS):\n",
    "  batch_losses_train, batch_losses_val = [], []\n",
    "\n",
    "  # iterate through the training data\n",
    "  for X_batch, y_batch in train_dataset:\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred_batch = regressor(X_batch) # TODO: pass current batch to the regressor to get the predictions\n",
    "        batch_loss = mse_loss(y_batch,y_pred_batch) # TODO: calculate the loss using the mse_loss function\n",
    "\n",
    "    # get the gradients of the weights with respect to the loss\n",
    "    grads = tape.gradient(batch_loss, regressor.variables)\n",
    "\n",
    "    # update the weights of the model\n",
    "    for g, v in zip(grads, regressor.variables):\n",
    "      v.assign_sub(LR*g) # TODO: update the variable by subtracting the product of the gradient and the learning rate using v.assign_sub\n",
    "    \n",
    "    # keep track of batch-level training performance \n",
    "    batch_losses_train.append(batch_loss)\n",
    "\n",
    "  # iterate through the validation data\n",
    "  for X_batch, y_batch in val_dataset:\n",
    "\n",
    "    y_pred_batch = regressor(X_batch) # TODO: pass current batch to the regressor to get the predictions\n",
    "    batch_loss = mse_loss(y_batch,y_pred_batch) # TODO: calculate the loss using the mse_loss function\n",
    "\n",
    "    # keep track of batch-level validation performance \n",
    "    batch_losses_val.append(batch_loss)\n",
    "\n",
    "  # keep track of epoch-level model performance\n",
    "  train_loss = tf.reduce_mean(batch_losses_train)\n",
    "  val_loss = tf.reduce_mean(batch_losses_val)\n",
    "  train_losses.append(train_loss)\n",
    "  val_losses.append(val_loss)\n",
    "  if epoch % 10 == 0:\n",
    "    print(f'Mean squared error for epoch {epoch}: {train_loss.numpy():0.3f}')\n",
    "\n",
    "# output final losses\n",
    "print(f\"\\nFinal train loss: {train_loss:0.3f}\")\n",
    "print(f\"Final validation loss: {val_loss:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63b6c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [9, 6]\n",
    "\n",
    "plt.plot(range(EPOCHS), train_losses, label = \"Training loss\")\n",
    "plt.plot(range(EPOCHS), val_losses, label = \"Validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean squared error loss\")\n",
    "plt.legend()\n",
    "plt.title(\"MSE loss vs training iterations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f44cd57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regressor(X_test_norm) # TODO: make predictions on the X_test_norm data using the regressor (a numpy array)\n",
    "submission = pd.DataFrame(predictions, columns=[\"Admission\"])\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01fc5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regressor(X_test_norm).numpy()# TODO: make predictions on the X_test_norm data using the regressor (a numpy array)\n",
    "submission = pd.DataFrame(predictions, columns=[\"Admission\"])\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df91b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5 # TODO: set the batch size to your preference\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_norm, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=X_train_norm.shape[0]).batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_norm, y_val))\n",
    "val_dataset = val_dataset.shuffle(buffer_size=X_val_norm.shape[0]).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3285688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "EPOCHS = 120 # TODO: set the number of epochs to your preference\n",
    "LR = 0.001 # TODO: set the learning rate to your preference\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "# create the model\n",
    "regressor = LinearRegression()\n",
    "\n",
    "# train the model\n",
    "for epoch in range(EPOCHS):\n",
    "  batch_losses_train, batch_losses_val = [], []\n",
    "\n",
    "  # iterate through the training data\n",
    "  for X_batch, y_batch in train_dataset:\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred_batch = regressor(X_batch) # TODO: pass current batch to the regressor to get the predictions\n",
    "        batch_loss = mse_loss(y_batch,y_pred_batch) # TODO: calculate the loss using the mse_loss function\n",
    "\n",
    "    # get the gradients of the weights with respect to the loss\n",
    "    grads = tape.gradient(batch_loss, regressor.variables)\n",
    "\n",
    "    # update the weights of the model\n",
    "    for g, v in zip(grads, regressor.variables):\n",
    "      v.assign_sub(LR*g) # TODO: update the variable by subtracting the product of the gradient and the learning rate using v.assign_sub\n",
    "    \n",
    "    # keep track of batch-level training performance \n",
    "    batch_losses_train.append(batch_loss)\n",
    "\n",
    "  # iterate through the validation data\n",
    "  for X_batch, y_batch in val_dataset:\n",
    "\n",
    "    y_pred_batch = regressor(X_batch) # TODO: pass current batch to the regressor to get the predictions\n",
    "    batch_loss = mse_loss(y_batch,y_pred_batch) # TODO: calculate the loss using the mse_loss function\n",
    "\n",
    "    # keep track of batch-level validation performance \n",
    "    batch_losses_val.append(batch_loss)\n",
    "\n",
    "  # keep track of epoch-level model performance\n",
    "  train_loss = tf.reduce_mean(batch_losses_train)\n",
    "  val_loss = tf.reduce_mean(batch_losses_val)\n",
    "  train_losses.append(train_loss)\n",
    "  val_losses.append(val_loss)\n",
    "  if epoch % 10 == 0:\n",
    "    print(f'Mean squared error for epoch {epoch}: {train_loss.numpy():0.3f}')\n",
    "\n",
    "# output final losses\n",
    "print(f\"\\nFinal train loss: {train_loss:0.3f}\")\n",
    "print(f\"Final validation loss: {val_loss:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c25549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [9, 6]\n",
    "\n",
    "plt.plot(range(EPOCHS), train_losses, label = \"Training loss\")\n",
    "plt.plot(range(EPOCHS), val_losses, label = \"Validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean squared error loss\")\n",
    "plt.legend()\n",
    "plt.title(\"MSE loss vs training iterations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "130da063",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regressor(X_test_norm).numpy()# TODO: make predictions on the X_test_norm data using the regressor (a numpy array)\n",
    "submission = pd.DataFrame(predictions, columns=[\"Admission\"])\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e276f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_b = regressor.variables[0].numpy().shape\n",
    "shape_w = regressor.variables[1].numpy().shape\n",
    "print(\"Bias shape:\", shape_b)\n",
    "print(\"Weights shape:\", shape_w)\n",
    "\n",
    "shape_mean = normalizer.mean.numpy().shape\n",
    "shape_std = normalizer.std.numpy().shape\n",
    "print(\"Mean shape:\", shape_mean)\n",
    "print(\"Standard deviation shape:\", shape_std)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
